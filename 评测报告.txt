
之前的带情感的对话生成的模型测试结果经常会出现提问和回答不匹配的问题，类似如下示例：

1.
post:  寻人启示寻什么呐  response:  寻校园微博达人。

生成各种情感的回复：
null:  一个的人
like:  对的对啊，我对啊对想的！
sad:  是，我也不知道啊
disgust:  对啊真可怕
angry:  这是被逼的
happy:  哈哈，这的

2.

post:  阿根廷的路牌。哈哈，太幽默了！  response:  有木有想起那个咸蛋超人和面包超人的地理题

生成各种情感的回复：
null:  是的是的是的是的种罪
like:  是的啊！
sad:  哎啊，，惨啊！
disgust:  中国啊，中国都中国这样的，中国人人的做法也是一？
angry:  是啊以为啊，死还以为死啊
happy:  哈哈哈哈哈哈的

3.
post:  多么清心寡欲的男人，才会有这样的卧室啊。  response:  电脑，电视机，电话，音响也不要吗？

生成各种情感的回复：
null:  一个人在一起！
like:  支持！
sad:  是，都吧不无言的
disgust:  扭曲的人，你这人有文化！
angry:  你是我了
happy:  哈哈，你！

目前还没有找到能够衡量post和response主题匹配的测试代码
我现在的想法是通过在对话语料上训练主题模型，利用主题模型分别给post和response分配当前最可能的主题模型
考虑到主题模型的准确性不高，取每个句子的前3个最可能的主题：

比如 主题模型分配给 post最可能的三个主题是：t1,t2,t3
                 给response最可能的三个主题是：t4,t3,t5

两者有交叉主题，比如t3 ,则认为主题效果好,分数+1，否则分数为0 
最终在整个测试集上得到所有的分数/测试集数量=最终主题匹配的准确率 来衡量主题相关性

测试模型的指标：
 通用指标：
				perplexity用来衡量句子是否通顺
				distinct值用来衡量句子的丰富度，间接可以反映句子的主题信息
				bleu 机器翻译评测指标
 
 针对任务的特殊指标：
				1.生成的情感语句的准确率，利用训练好的情感分类模型测试
				2.主题相关性，利用上述提到的 方法评测
				3.还可以利用bert 在对话语料上进行微调，因为bert有一个训练任务是判断两句话是否是前后对应关系
				  可以用来评测post和response的相关性
				  
现在还在写最后两个特殊指标的代码。				  
				  
				  
				  
				  
				  
				  
				  
				  				  